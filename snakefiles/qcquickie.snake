import re
import pandas
import ruamel.yaml

import pkg_resources
import sys
sys.path.append(os.path.join(os.path.dirname(workflow.snakefile), "../"))

import lib.serum as serum
# from pytools.persistent_dict import PersistentDict
# size estimation from kmers as well with jellyfish?
# storage = PersistentDict("qcquickie_storage")
# alternative is to do a quick qc on basepairs followed by a more rigorous assembly

configfile: os.path.join(os.path.dirname(workflow.snakefile), "../config/config.yaml")
# requires --config R1_reads={read_location},R2_reads={read_location}
sample = config["Sample"]
R1 = config["R1_reads"],
R2 = config["R2_reads"],

yaml = ruamel.yaml.YAML(typ='safe')
yaml.default_flow_style = False
with open(sample, "r") as yaml_stream:
    config_sample = yaml.load(yaml_stream)

folder_name = "qcquickie"
# my understanding is all helps specify final output
onsuccess:
    print("Workflow complete")
    output = ["qcquickie_status.txt"]
    with open(output[0], "w") as status:
        status.write("Success\n")
onerror:
    print("Workflow error")
    output = ["qcquickie_status.txt"]
    with open(output[0], "w") as status:
        status.write("Failure\n")

rule all:
    input:
        folder_name,
        os.path.join(folder_name, "contigs.bin.cov"),
        os.path.join(folder_name, "bracken.txt"),
        os.path.join(folder_name, "contaminantion_check.txt")


# rule run__qcquickie:
#     message:
#         "QC quickie"
#     input:
#         reads = (R1, R2)
#     output:
#         "qcquickie_done"
#     params:
#         options = "".join(config["tadpole"]["options"]),
#         adapter = "/tools/git.repositories/SerumQC-private/DB/adapters.fasta"
#     shell:
#         """
#         mkdir qcquickie
#         cd qcquickie
#         bbduk.sh in={input.reads[0]} in2={input.reads[1]} out=filtered.fastq ref={params.adapter} ktrim=r k=23 mink=11 hdist=1 tbo minavgquality=14 &>> qcquickie.log
#         kraken -db /srv/data/DB/kraken/minikraken_20171019_8GB/ --fastq-input filtered.fastq 2> kraken.log | kraken-report -db /srv/data/DB/kraken/minikraken_20171019_8GB/ 1> mini.report 2> kraken.log
#         python /tools/git.repositories/Bracken/est_abundance.py -i mini.report -k /srv/data/DB/kraken/minikraken_20171019_8GB/minikraken_8GB_100mers_distrib.txt -o bracken.txt &>> kraken.log
#         bbmerge.sh in=filtered.fastq out=merged.fastq outu=unmerged.fastq &>> qcquickie.log
#         tadpole.sh in=merged.fastq,unmerged.fastq out=contigs.fa &>> qcquickie.log
#         bbwrap.sh ref=contigs.fa in=merged.fastq,unmerged.fastq out=contigs.sam append &>> qcquickie.log
#         pileup.sh in=contigs.sam basecov=contigs.cov out=contigs.pileup &>> qcquickie.log
#         filterbycoverage.sh in=contigs.fa cov=contigs.pileup minc=10 &>> qcquickie.log
#         filterbycoverage.sh in=contigs.fa cov=contigs.pileup minc=25 &>> qcquickie.log
#         sketch.sh in=contigs.fa out=contigs.sketch &>> qcquickie.log
#         cd ..
#         touch qcquickie_done
#         """

rule setup:
    output:
        dir = folder_name
    shell:
        "mkdir {output}"


rule setup__filter_reads_with_bbduk:
    message:
        "Running step: {rule}"
    input:
        dir = folder_name,
        reads = (R1, R2)
    output:
        filtered_reads = os.path.join(folder_name, "filtered.fastq")
    params:
        adapters = os.path.join(os.path.dirname(workflow.snakefile), "../lib/adapters.fasta")
    log:
        os.path.join(folder_name, "setup__filter_reads_with_bbduk.log")
    shell:
        "bbduk.sh in={input.reads[0]} in2={input.reads[1]} out={output.filtered_reads} ref={params.adapters} ktrim=r k=23 mink=11 hdist=1 tbo minavgquality=14 &> {log}"


rule contaminant_check__classify_reads_kraken_minikraken_db:
    message:
        "Running step: {rule}"
    input:
        filtered_reads = os.path.join(folder_name, "filtered.fastq")
    output:
        kraken_report = os.path.join(folder_name, "kraken_report.txt")
    params:
        db = "/srv/data/DB/kraken/minikraken_20171019_8GB/"
    log:
        os.path.join(folder_name, "contaminant_check__classify_reads_kraken_minikraken_db.log")
    shell:
        "kraken -db {params.db} --fastq-input {input.filtered_reads} 2> {log} | kraken-report -db {params.db} 1> {output.kraken_report}"


rule contaminant_check__determine_species_bracken_on_minikraken_results:
    message:
        "Running step: {rule}"
    input:
        kraken_report = os.path.join(folder_name, "kraken_report.txt")
    output:
        bracken = os.path.join(folder_name, "bracken.txt")
    log:
        os.path.join(folder_name, "contaminant_check__determine_species_bracken_on_minikraken_results.log")
    params:
        kmer_dist = "/srv/data/DB/kraken/minikraken_20171019_8GB/minikraken_8GB_100mers_distrib.txt"
    shell:
        """
        est_abundance.py -i {input.kraken_report} -k {params.kmer_dist} -o {output.bracken} &> {log}
        sort -r -t$'\t' -k7 {output.bracken} -o {output.bracken}
        """


rule assembly_check__combine_reads_with_bbmerge:
    message:
        "Running step: {rule}"
    input:
        filtered_reads = os.path.join(folder_name, "filtered.fastq")
    output:
        merged_reads = os.path.join(folder_name, "merged.fastq"),
        unmerged_reads = os.path.join(folder_name, "unmerged.fastq")
    log:
        os.path.join(folder_name, "assembly_check__combine_reads_with_bbmerge.log")
    shell:
        "bbmerge.sh in={input.filtered_reads} out={output.merged_reads} outu={output.unmerged_reads} &> {log}"


rule assembly_check__quick_assembly_with_tadpole:
    message:
        "Running step: {rule}"
    input:
        merged_reads = os.path.join(folder_name, "merged.fastq"),
        unmerged_reads = os.path.join(folder_name, "unmerged.fastq")
    output:
        contigs = os.path.join(folder_name, "contigs.fasta")
    log:
        os.path.join(folder_name, "assembly_check__quick_assembly_with_tadpole.log")
    shell:
        "tadpole.sh in={input.merged_reads},{input.unmerged_reads} out={output.contigs} &> {log}"


rule assembly_check__map_reads_to_assembly_with_bbwrap:
    message:
        "Running step: {rule}"
    input:
        contigs = os.path.join(folder_name, "contigs.fasta"),
        merged_reads = os.path.join(folder_name, "merged.fastq"),
        unmerged_reads = os.path.join(folder_name, "unmerged.fastq")
    output:
        mapped = os.path.join(folder_name, "contigs.sam")
    log:
        os.path.join(folder_name, "assembly_check__map_reads_to_assembly_with_bbwrap.log")
    shell:
        "bbwrap.sh ref={input.contigs} in={input.merged_reads},{input.unmerged_reads} out={output.mapped} append &> {log}"


rule assembly_check__pileup_on_mapped_reads:
    message:
        "Running step: {rule}"
    input:
        mapped = os.path.join(folder_name, "contigs.sam")
    output:
        coverage = os.path.join(folder_name, "contigs.cov"),
        pileup = os.path.join(folder_name, "contigs.pileup")
    log:
        os.path.join(folder_name, "assembly_check__pileup_on_mapped_reads.log")
    shell:
        "pileup.sh in={input.mapped} basecov={output.coverage} out={output.pileup} &> {log}"


rule assembly_check__bin_coverage:
    message:
        "Running step: {rule}"
    input:
        coverage = os.path.join(folder_name, "contigs.cov")
    output:
        contig_depth_yaml = os.path.join(folder_name, "contigs.sum.cov"),
        binned_depth_yaml = os.path.join(folder_name, "contigs.bin.cov")
    run:
        serum.script__summarize_depth(input.coverage, output.contig_depth_yaml, output.binned_depth_yaml)


rule contaminant_check__declare_contamination:
    message:
        "Running step: {rule}"
    input:
        bracken = os.path.join(folder_name, "bracken.txt")
    output:
        contaminantion_check = os.path.join(folder_name, "contaminantion_check.txt")
    run:
        with open(output.contaminantion_check, "w") as contaminantion_check:
            df = pandas.read_table(input.bracken)
            if df[df["fraction_total_reads"] > 0.05].shape[0] == 1:
                contaminantion_check.write("No contaminant detected\n")
            else:
                contaminantion_check.write("Contaminant found or Error")
