import re
import pandas
import ruamel.yaml

import pkg_resources
import sys

# from pytools.persistent_dict import PersistentDict
# size estimation from kmers as well with jellyfish?
# storage = PersistentDict("qcquickie_storage")
# alternative is to do a quick qc on basepairs followed by a more rigorous assembly

configfile: os.path.join(os.path.dirname(workflow.snakefile), "../config.yaml")
# requires --config R1_reads={read_location},R2_reads={read_location}
sample = config["Sample"]
R1 = config["R1_reads"]
R2 = config["R2_reads"]
global_threads = config["global"]["threads"]
global_memory_in_GB = config["global"]["memory"]

yaml = ruamel.yaml.YAML(typ='safe')
yaml.default_flow_style = False
with open(sample, "r") as yaml_stream:
    config_sample = yaml.load(yaml_stream)

folder_name = "qcquickie"
# my understanding is all helps specify final output
onsuccess:
    print("Workflow complete")
    output = ["qcquickie_status.txt"]
    with open(output[0], "w") as status:
        status.write("Success\n")
onerror:
    print("Workflow error")
    output = ["qcquickie_status.txt"]
    with open(output[0], "w") as status:
        status.write("Failure\n")


rule all:
    input:
        folder_name,
        os.path.join(folder_name, "contigs.bin.cov"),
        os.path.join(folder_name, "bracken.txt"),
        os.path.join(folder_name, "contaminantion_check.txt")


rule setup:
    output:
        dir = folder_name
    shell:
        "mkdir {output}"


rule setup__filter_reads_with_bbduk:
    message:
        "Running step: {rule}"
    input:
        dir = folder_name,
        reads = (R1, R2)
    output:
        filtered_reads = os.path.join(folder_name, "filtered.fastq")
    params:
        adapters = os.path.join(os.path.dirname(workflow.snakefile), "../lib/adapters.fasta")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/setup__filter_reads_with_bbduk.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/setup__filter_reads_with_bbduk.benchmark")
    shell:
        "bbduk.sh threads={threads} -Xmx{resources.memory_in_GB}G in={input.reads[0]} in2={input.reads[1]} out={output.filtered_reads} ref={params.adapters} ktrim=r k=23 mink=11 hdist=1 tbo minavgquality=14 &> {log}"


rule contaminant_check__classify_reads_kraken_minikraken_db:
    message:
        "Running step: {rule}"
    input:
        filtered_reads = os.path.join(folder_name, "filtered.fastq")
    output:
        kraken_report = os.path.join(folder_name, "kraken_report.txt")
    params:
        db = "/srv/data/DB/kraken/minikraken_20171019_8GB/"
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/contaminant_check__classify_reads_kraken_minikraken_db.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/contaminant_check__classify_reads_kraken_minikraken_db.benchmark")
    shell:
        "kraken --threads {threads} -db {params.db} --fastq-input {input.filtered_reads} 2> {log} | kraken-report -db {params.db} 1> {output.kraken_report}"


rule contaminant_check__determine_species_bracken_on_minikraken_results:
    message:
        "Running step: {rule}"
    input:
        kraken_report = os.path.join(folder_name, "kraken_report.txt")
    output:
        bracken = os.path.join(folder_name, "bracken.txt")
    params:
        kmer_dist = "/srv/data/DB/kraken/minikraken_20171019_8GB/minikraken_8GB_100mers_distrib.txt"
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/contaminant_check__determine_species_bracken_on_minikraken_results.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/contaminant_check__determine_species_bracken_on_minikraken_results.benchmark")
    shell:
        """
        est_abundance.py -i {input.kraken_report} -k {params.kmer_dist} -o {output.bracken} &> {log}
        sort -r -t$'\t' -k7 {output.bracken} -o {output.bracken}
        """


rule assembly_check__combine_reads_with_bbmerge:
    message:
        "Running step: {rule}"
    input:
        filtered_reads = os.path.join(folder_name, "filtered.fastq")
    output:
        merged_reads = os.path.join(folder_name, "merged.fastq"),
        unmerged_reads = os.path.join(folder_name, "unmerged.fastq")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__combine_reads_with_bbmerge.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__combine_reads_with_bbmerge.benchmark")
    shell:
        "bbmerge.sh threads={threads} -Xmx{resources.memory_in_GB}G in={input.filtered_reads} out={output.merged_reads} outu={output.unmerged_reads} &> {log}"


rule assembly_check__quick_assembly_with_tadpole:
    message:
        "Running step: {rule}"
    input:
        merged_reads = os.path.join(folder_name, "merged.fastq"),
        unmerged_reads = os.path.join(folder_name, "unmerged.fastq")
    output:
        contigs = os.path.join(folder_name, "contigs.fasta")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__quick_assembly_with_tadpole.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__quick_assembly_with_tadpole.benchmark")
    shell:
        "tadpole.sh threads={threads} -Xmx{resources.memory_in_GB}G in={input.merged_reads},{input.unmerged_reads} out={output.contigs} &> {log}"


rule assembly_check__map_reads_to_assembly_with_bbwrap:
    message:
        "Running step: {rule}"
    input:
        contigs = os.path.join(folder_name, "contigs.fasta"),
        merged_reads = os.path.join(folder_name, "merged.fastq"),
        unmerged_reads = os.path.join(folder_name, "unmerged.fastq")
    output:
        mapped = os.path.join(folder_name, "contigs.sam")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__map_reads_to_assembly_with_bbwrap.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__map_reads_to_assembly_with_bbwrap.benchmark")
    shell:
        "bbwrap.sh threads={threads} -Xmx{resources.memory_in_GB}G ref={input.contigs} in={input.merged_reads},{input.unmerged_reads} out={output.mapped} append &> {log}"


rule assembly_check__pileup_on_mapped_reads:
    message:
        "Running step: {rule}"
    input:
        mapped = os.path.join(folder_name, "contigs.sam")
    output:
        coverage = os.path.join(folder_name, "contigs.cov"),
        pileup = os.path.join(folder_name, "contigs.pileup")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__pileup_on_mapped_reads.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__pileup_on_mapped_reads.benchmark")
    shell:
        "pileup.sh threads={threads} -Xmx{resources.memory_in_GB}G in={input.mapped} basecov={output.coverage} out={output.pileup} &> {log}"


rule assembly_check__bin_coverage:
    message:
        "Running step: {rule}"
    input:
        coverage = os.path.join(folder_name, "contigs.cov")
    output:
        contig_depth_yaml = os.path.join(folder_name, "contigs.sum.cov"),
        binned_depth_yaml = os.path.join(folder_name, "contigs.bin.cov")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/assembly_check__bin_coverage.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/assembly_check__bin_coverage.benchmark")
    script:
        "scripts/summarize_depth.py"
    #run:
        #serum.script__summarize_depth(input.coverage, output.contig_depth_yaml, output.binned_depth_yaml)


rule contaminant_check__declare_contamination:
    message:
        "Running step: {rule}"
    input:
        bracken = os.path.join(folder_name, "bracken.txt")
    output:
        contaminantion_check = os.path.join(folder_name, "contaminantion_check.txt")
    threads:
        global_threads
    resources:
        memory_in_GB = global_memory_in_GB
    log:
        os.path.join(folder_name, "log/contaminant_check__declare_contamination.log")
    benchmark:
        os.path.join(folder_name, "benchmarks/contaminant_check__declare_contamination.benchmark")
    run:
        with open(output.contaminantion_check, "w") as contaminantion_check:
            df = pandas.read_table(input.bracken)
            if df[df["fraction_total_reads"] > 0.05].shape[0] == 1:
                contaminantion_check.write("No contaminant detected\n")
            else:
                contaminantion_check.write("Contaminant found or Error")
